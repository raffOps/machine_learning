{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.Kfold.kfold import Kfold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/winequality-red.csv\", sep=\";\")\n",
    "y = df.quality.apply(lambda quality: 0 if quality <= 5 else 1)\n",
    "X =  MinMaxScaler().fit_transform(df.iloc[:,:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_random_combinations_of_parameters(\n",
    "        parameters: dict,\n",
    "        number_of_combinations: int\n",
    ") -> list[int]:\n",
    "    combinations = []\n",
    "    for _ in range(number_of_combinations):\n",
    "        combination = {}\n",
    "        for parameter, values in parameters.items():\n",
    "            combination[parameter] = random.choice(values)\n",
    "        combinations.append(combination)\n",
    "    return combinations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "\n",
    "K = 10\n",
    "validation_kfold = Kfold(K)\n",
    "for (x_train_validation,\n",
    "     x_test_validation,\n",
    "     y_train_validation,\n",
    "     y_test_validation) in validation_kfold.split(X, y):\n",
    "    tunning_kfold = Kfold(K-1)\n",
    "    for (x_train_tunning,\n",
    "         x_test_tunning,\n",
    "         y_train_tunning,\n",
    "         y_test_tunning) in tunning_kfold.split(x_train_validation, y_train_validation):\n",
    "        parameters_combinations = get_random_combinations_of_parameters(\n",
    "            random_forest_parameters_grid,\n",
    "            20\n",
    "        )\n",
    "        for combination in parameters_combinations:\n",
    "            inicial_time = time()\n",
    "            cls = RandomForestClassifier(**parameters_combinations)\n",
    "            final_time = time()\n",
    "            cls.fit(x_train_tunning, y_train_tunning)\n",
    "            predictions = cls.predict(x_test_tunning)\n",
    "            score = f1_score(y_test_tunning, predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "random.seed(44)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "a = Kfold(3)\n",
    "a = list(a.split(X, y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "cls = RandomForestClassifier\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 300],\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.2, None]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_tunned_parameters(cls, parameters_grid, numbers_of_folds, number_of_parameters_combinations):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_grid_tunning(classifier, parameters_grid, numbers_of_folds, number_of_parameters_combinations):\n",
    "    grid_results = []\n",
    "    parameters_combinations = get_random_combinations_of_parameters(\n",
    "        parameters_grid,\n",
    "        number_of_parameters_combinations\n",
    "    )\n",
    "    for combination in parameters_combinations:\n",
    "        tunning_kfold = Kfold(numbers_of_folds)\n",
    "        for (\n",
    "            x_train_tunning,\n",
    "            x_test_tunning,\n",
    "            y_train_tunning,\n",
    "            y_test_tunning\n",
    "        ) in tunning_kfold.split(X, y):\n",
    "            times = []\n",
    "            scores = []\n",
    "            initial_time = time()\n",
    "            cls = classifier(**combination)\n",
    "            cls.fit(x_train_tunning, y_train_tunning)\n",
    "            final_time = time()\n",
    "            times.append(final_time-initial_time)\n",
    "            predictions = cls.predict(x_test_tunning)\n",
    "            score = f1_score(y_test_tunning, predictions)\n",
    "            scores.append(score)\n",
    "        mean_time = np.asarray(times).mean()\n",
    "        scores = np.asarray(scores)\n",
    "        mean_score = scores.mean()\n",
    "        std_score = scores.std()\n",
    "    #     print(f\"\"\"index combination: {index_combination},\n",
    "    # \\tparameters = {combination}\n",
    "    # \\tmean score: {mean_score}\n",
    "    # \\tstd score: {std_score}\"\"\")\n",
    "        grid_results.append(\n",
    "            dict(combination, f1_mean_score=mean_score, f1_score_std=std_score, mean_time=mean_time)\n",
    "        )\n",
    "    grid_results = pd.DataFrame(grid_results).sort_values(\n",
    "        by=[\"f1_mean_score\"],\n",
    "        ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return grid_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "random_forest_parameters_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 300],\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.2, None]\n",
    "}\n",
    "grid_results = []\n",
    "K = 10\n",
    "parameters_combinations = get_random_combinations_of_parameters(\n",
    "    random_forest_parameters_grid,\n",
    "    20\n",
    ")\n",
    "for combination in parameters_combinations:\n",
    "    tunning_kfold = Kfold(K)\n",
    "    for (\n",
    "        x_train_tunning,\n",
    "        x_test_tunning,\n",
    "        y_train_tunning,\n",
    "        y_test_tunning\n",
    "    ) in tunning_kfold.split(X, y):\n",
    "        times = []\n",
    "        scores = []\n",
    "        initial_time = time()\n",
    "        cls = RandomForestClassifier(**combination)\n",
    "        cls.fit(x_train_tunning, y_train_tunning)\n",
    "        final_time = time()\n",
    "        times.append(final_time-initial_time)\n",
    "        predictions = cls.predict(x_test_tunning)\n",
    "        score = f1_score(y_test_tunning, predictions)\n",
    "        scores.append(score)\n",
    "    mean_time = np.asarray(times).mean()\n",
    "    scores = np.asarray(scores)\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "#     print(f\"\"\"index combination: {index_combination},\n",
    "# \\tparameters = {combination}\n",
    "# \\tmean score: {mean_score}\n",
    "# \\tstd score: {std_score}\"\"\")\n",
    "    grid_results.append(\n",
    "        dict(combination, f1_mean_score=mean_score, f1_score_std=std_score, mean_time=mean_time)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "random_forest_tunning_results = pd.DataFrame(grid_results).sort_values(by=[\"f1_mean_score\"], ascending=False).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "random_forest_tunning_results.to_csv(\"../data/results/random_search_tunning_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "    n_estimators criterion max_features  f1_mean_score  f1_score_std  \\\n0            300  log_loss          0.2       0.897436           0.0   \n1            300      gini          0.2       0.875000           0.0   \n2             50      gini         sqrt       0.835616           0.0   \n3            100   entropy         sqrt       0.833333           0.0   \n4            300   entropy          0.2       0.832215           0.0   \n5            300      gini          0.2       0.826667           0.0   \n6            300  log_loss         log2       0.823529           0.0   \n7            300      gini         log2       0.813793           0.0   \n8             10  log_loss         sqrt       0.811594           0.0   \n9             10  log_loss         None       0.811189           0.0   \n10            10  log_loss         None       0.810811           0.0   \n11           300   entropy          0.2       0.808511           0.0   \n12           300      gini         log2       0.802817           0.0   \n13           100      gini         sqrt       0.794702           0.0   \n14           300   entropy         sqrt       0.772414           0.0   \n15            10   entropy          0.2       0.737589           0.0   \n16            10  log_loss          0.2       0.737589           0.0   \n17            50  log_loss         None       0.722222           0.0   \n18            10   entropy         sqrt       0.718310           0.0   \n19            10      gini          0.2       0.709220           0.0   \n\n    mean_time  \n0    0.851525  \n1    0.598849  \n2    0.124862  \n3    0.259981  \n4    0.655196  \n5    0.606480  \n6    0.770371  \n7    0.706848  \n8    0.027084  \n9    0.056491  \n10   0.054382  \n11   0.661252  \n12   0.688013  \n13   0.238707  \n14   0.822530  \n15   0.023396  \n16   0.025783  \n17   0.269271  \n18   0.027783  \n19   0.021823  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>criterion</th>\n      <th>max_features</th>\n      <th>f1_mean_score</th>\n      <th>f1_score_std</th>\n      <th>mean_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>300</td>\n      <td>log_loss</td>\n      <td>0.2</td>\n      <td>0.897436</td>\n      <td>0.0</td>\n      <td>0.851525</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>300</td>\n      <td>gini</td>\n      <td>0.2</td>\n      <td>0.875000</td>\n      <td>0.0</td>\n      <td>0.598849</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>50</td>\n      <td>gini</td>\n      <td>sqrt</td>\n      <td>0.835616</td>\n      <td>0.0</td>\n      <td>0.124862</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100</td>\n      <td>entropy</td>\n      <td>sqrt</td>\n      <td>0.833333</td>\n      <td>0.0</td>\n      <td>0.259981</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>300</td>\n      <td>entropy</td>\n      <td>0.2</td>\n      <td>0.832215</td>\n      <td>0.0</td>\n      <td>0.655196</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>300</td>\n      <td>gini</td>\n      <td>0.2</td>\n      <td>0.826667</td>\n      <td>0.0</td>\n      <td>0.606480</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>300</td>\n      <td>log_loss</td>\n      <td>log2</td>\n      <td>0.823529</td>\n      <td>0.0</td>\n      <td>0.770371</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>300</td>\n      <td>gini</td>\n      <td>log2</td>\n      <td>0.813793</td>\n      <td>0.0</td>\n      <td>0.706848</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>log_loss</td>\n      <td>sqrt</td>\n      <td>0.811594</td>\n      <td>0.0</td>\n      <td>0.027084</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>log_loss</td>\n      <td>None</td>\n      <td>0.811189</td>\n      <td>0.0</td>\n      <td>0.056491</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>log_loss</td>\n      <td>None</td>\n      <td>0.810811</td>\n      <td>0.0</td>\n      <td>0.054382</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>300</td>\n      <td>entropy</td>\n      <td>0.2</td>\n      <td>0.808511</td>\n      <td>0.0</td>\n      <td>0.661252</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>300</td>\n      <td>gini</td>\n      <td>log2</td>\n      <td>0.802817</td>\n      <td>0.0</td>\n      <td>0.688013</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>100</td>\n      <td>gini</td>\n      <td>sqrt</td>\n      <td>0.794702</td>\n      <td>0.0</td>\n      <td>0.238707</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>300</td>\n      <td>entropy</td>\n      <td>sqrt</td>\n      <td>0.772414</td>\n      <td>0.0</td>\n      <td>0.822530</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>10</td>\n      <td>entropy</td>\n      <td>0.2</td>\n      <td>0.737589</td>\n      <td>0.0</td>\n      <td>0.023396</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>10</td>\n      <td>log_loss</td>\n      <td>0.2</td>\n      <td>0.737589</td>\n      <td>0.0</td>\n      <td>0.025783</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>50</td>\n      <td>log_loss</td>\n      <td>None</td>\n      <td>0.722222</td>\n      <td>0.0</td>\n      <td>0.269271</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>10</td>\n      <td>entropy</td>\n      <td>sqrt</td>\n      <td>0.718310</td>\n      <td>0.0</td>\n      <td>0.027783</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>10</td>\n      <td>gini</td>\n      <td>0.2</td>\n      <td>0.709220</td>\n      <td>0.0</td>\n      <td>0.021823</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_tunning_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}